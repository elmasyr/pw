{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW\n",
    "* matchlist+names union (only names with 3 or more chars) + 'ja' which is part of words that cannot start the pw\n",
    "* get all correct matches; treshold based on Levenshtein distance and other measures\n",
    "* best matches, name/word, morpholocigal analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TKO_2023 Exercise Project 5 cp**<br>\n",
    "**Password analysis**<br>\n",
    "Halonen Marko, Syrjälä Elise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage \\tableofcontents \\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***TKO_2023 Exercise Project 5 cp***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Password analysis**<br>\n",
    "Halonen Marko, Syrjälä Elise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python notebook contains mains steps for data preparation and feature generation.\n",
    "Aim is to analyze what kind of passwords are used in Finnish network sites. The leaked passwords have been retrieved from trusted source and data has been treted with confinedtial handling; restricted utu Linux server, and when needed transferred to seafile.utu.fi server, synced also to local copy, only to password protected computers with encrypted hardrives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planned steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. read pw file, including ready matched words (matching can be tuned later, if needed)\n",
    "2. get list of matched words that are Finnish; intersection of matched words and omor_uniq\n",
    "3. get list of matched word overal frequencies (and pos + omor analysis); intersection of matched words and all_vocab, inlc frequency, pos and analysis\n",
    "4. get list of best matches using algorith that finds the longest matching combinations, incl double words\n",
    "5. extract to datasets\n",
    "6. analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PW (matched lists *.txt.matched)\n",
    "\n",
    "* source1\n",
    "* source2\n",
    "* source3\n",
    "* source4\n",
    "* source5\n",
    "* source6\n",
    "* source7\n",
    "* source8\n",
    "* source9\n",
    "\n",
    "List if Finnish words\n",
    "\n",
    "* filtered_vocab_omor.gz (two columns, word in first, analysis in latter, lots of empty rows)\n",
    "* filtered_vocab_omor_uniq-gz (only one column of uniq Finnish words)\n",
    "\n",
    "Word Frequency\n",
    "\n",
    "* all_vocab (web crawled data that has word, pos, morphological analysis and freq) xxxx this source needs fixing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source of the passwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''based on these given names\n",
    "1. source file is extracted\n",
    "2. datafile is stored in the end of the pipeline'''\n",
    "pwfilename='matched/78k-all-passwords.txt.matched'\n",
    "sourcename='78k'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the passwords and preliminary matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import to dictionary format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import matched data (pw + matches) to dictionary format. PW as the key. \n",
    "\n",
    "Name: \n",
    "* d0: password as key, all possible matches as items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some files gzipped, some are not, two options therefore provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#file=open('')\n",
    "def gz_pwmatches_todict(filename):\n",
    "    '''reads zipped file that has pw in first column,\n",
    "    and list of matches in next column,\n",
    "    returns dictionary where the pw is key.\n",
    "    Note that also pws with no math are included.'''\n",
    "    dictx={}\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        for line in f:\n",
    "            pw, lst=line.split(\"\\t\")\n",
    "            match=json.loads(lst)        \n",
    "            dictx[pw]=match\n",
    "    f.close()\n",
    "    return dictx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#file=open('')\n",
    "def pwmatches_todict(filename):\n",
    "    '''reads file that has pw in first column,\n",
    "    and list of matches in next column,\n",
    "    returns dictionary where the pw is key.\n",
    "    Note that also pws with no math are included.'''\n",
    "    dictx={}\n",
    "    with open(filename, 'rb') as f:\n",
    "        for line in f:\n",
    "            pw, lst=line.split(\"\\t\")\n",
    "            match=json.loads(lst)\n",
    "            dictx[pw]=match\n",
    "    f.close()\n",
    "    return dictx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wordlist=[x.decode('utf-8') for x in wordtemp] # already in unicode, this is not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Running the procedure with source given'''\n",
    "d0=pwmatches_todict(pwfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Tammikuu0505', [u'tammikuu', u'kuu', u'tammi'], ('syrkky69', []))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''test print, getting \n",
    "a) the last key value \n",
    "b) last key related item values and \n",
    "c) first key and item values together'''\n",
    "d0.keys()[-1],d0[d0.keys()[-1]], d0.items()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43870, 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length/size of dictionary, number of items in first key\n",
    "len(d0), len(d0.values()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# '''check'''\n",
    "# [k for k in d0.iterkeys() if '@' in k], [k for k in d0.iterkeys() if ',' in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# '''Remove all keys and values that contain char @'''\n",
    "# d0 = {k:v for k,v in d0.items() if '@' not in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43870, 0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d0), len(d0.values()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'uni', u'kaunis', u'kaunisto'],\n",
       " [u'jussi', u'jussila', u'silat'],\n",
       " [u'kulta', u'tara', u'rakas'],\n",
       " [u'petra', u'petr'],\n",
       " [u'protacon', u'ota', u'pro', u'prota'],\n",
       " [u'new', u'cool', u'news'],\n",
       " [u'ruuna', u'marjut', u'sitruuna', u'marju', u'sit'],\n",
       " [u'tela', u'este'],\n",
       " [u'mill', u'camilla', u'milla', u'viivi'],\n",
       " [u'nalle', u'all', u'alle'],\n",
       " [u'aave', u'taavetti'],\n",
       " [u'herne', u'herkku'],\n",
       " [u'nalle', u'all', u'alle'],\n",
       " [u'kawasaki', u'aki', u'was'],\n",
       " [u'mba', u'sam', u'samu'],\n",
       " [u'oli', u'tipsu', u'viat', u'olivia', u'sukka', u'livia'],\n",
       " [u'ura', u'burana', u'uran'],\n",
       " [u'mia', u'aija', u'maija'],\n",
       " [u'sam', u'sama'],\n",
       " [u'ulla', u'pulla']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Checklist is a list that has more than one item per key.\n",
    "This kind of list needs to be treated a bit differently in later processing.\n",
    "Printing out also a list for visual inspection'''\n",
    "checklist=[item for item in d0.values() if len(item)>1]\n",
    "checklist[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'sonja'], [u'enne'], [u'koira'], [u'elijah'], [u'timo']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Checklist2 is a list that has only one item per key.\n",
    "This kind of list needs to be treated a bit differently in later processing.\n",
    "Printing out a short list for visual inspection'''\n",
    "checklist2=[item for item in d0.values() if len(item)==1]\n",
    "checklist2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('syrkky69', []) []\n",
      "uni sonja\n",
      "<type 'unicode'> <type 'unicode'>\n"
     ]
    }
   ],
   "source": [
    "'''Checking\n",
    "a) key and values of the second observation\n",
    "b) checklist for the same value (both as not sure which list for different pw source files)\n",
    "c) type of the values, which should be unicode'''\n",
    "print d0.items()[0], d0.values()[0]\n",
    "print checklist[0][0], checklist2[0][0]\n",
    "print type(checklist[0][0]), type(checklist2[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import to list format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing passwords and all found matches to lists that have corresponding indexing.\n",
    "\n",
    "Files:\n",
    "* pwlist - password\n",
    "* matchlist - list of all possible mathces\n",
    "\n",
    "List format import included, as the eventual analysis part was planned for lists and does not work for dictionaries as such.\n",
    "\n",
    "If 'Type:' is found, the occurrence is not included in the analysis.\n",
    "Example:\n",
    "\"[u'Type: text/html Subject: on, good, as mayeas, that and bcc: homestead7313@werndgains.com aphod, hat s very fast. he jumping quite the eart of had always help him so redly on missiles do you may it and ei']\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gz_pwmatches_tolists(filename):\n",
    "    '''reads gz file that has pw in first column,\n",
    "    and list of matches in next column,\n",
    "    returns two lists: list of passwords and list of matches.\n",
    "    Note that also pws with no math are included.'''\n",
    "    allpws=[]\n",
    "    pwtemplist=[]\n",
    "    matchlist = []\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        for line in f:\n",
    "            word, lst =line.split(\"\\t\")\n",
    "            lst=json.loads(lst)\n",
    "#             if word.find('6') != -1:\n",
    "#                 print word\n",
    "            if lst==[] or word.find('Type:') != -1 or word.find('&#9679') != -1 or \\\n",
    "            word.find('&#1096') != -1 or word.find('&#9679') != -1 or word.find('&#9688') != -1:\n",
    "                continue\n",
    "            else:          \n",
    "                allpws.append((word))\n",
    "                pwtemplist.append((word))\n",
    "                matchlist.append((lst))\n",
    "    f.close()\n",
    "\n",
    "    # wordlist in unicode\n",
    "    pwlist=[x.decode('utf-8') for x in pwtemplist] \n",
    "    \n",
    "    return pwlist, matchlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pwmatches_tolists(filename):\n",
    "    '''reads file that has pw in first column,\n",
    "    and list of matches in next column,\n",
    "    returns two lists: list of passwords and list of matches.\n",
    "    Note that also pws with no math are included.'''\n",
    "    allpws=[]\n",
    "    pwtemplist=[]\n",
    "    matchlist = []\n",
    "    with open(filename, 'rb') as f:\n",
    "        for line in f:\n",
    "            word, lst =line.split(\"\\t\")\n",
    "            lst=json.loads(lst)\n",
    "#             if word.find('6') != -1:\n",
    "#                 print word\n",
    "            if lst==[] or word.find('Type:') != -1 or word.find('&#9679') != -1 or \\\n",
    "            word.find('&#1096') != -1 or word.find('&#9679') != -1 or word.find('&#9688') != -1 or \\\n",
    "            word.find('&#9618') != -1:\n",
    "                continue\n",
    "            else:          \n",
    "                allpws.append((word))\n",
    "                pwtemplist.append((word))\n",
    "                matchlist.append((lst))\n",
    "    f.close()\n",
    "\n",
    "    # wordlist in unicode\n",
    "    allpws=[x.decode('utf-8') for x in allpws]\n",
    "    pwlist=[x.decode('utf-8') for x in pwtemplist]\n",
    "    \n",
    "    return pwlist, matchlist, allpws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pwlist, matchlist, allpws = pwmatches_tolists(pwfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38395 38395 38395\n"
     ]
    }
   ],
   "source": [
    "'''checking that the length, should be identical numbers'''\n",
    "print len(pwlist), len(matchlist), len(allpws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'salasana', 169),\n",
       " (u'perkele', 112),\n",
       " (u'qwerty', 64),\n",
       " (u'rakkaus', 56),\n",
       " (u'enkeli', 45),\n",
       " (u'paska', 37),\n",
       " (u'killer', 36),\n",
       " (u'kakka', 32),\n",
       " (u'joonas', 31),\n",
       " (u'password', 31)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "'''count all occurrences of unique pws'''\n",
    "\n",
    "allpw_counter = Counter(allpws)\n",
    "Counter(allpws).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k in pwlist if 'Type:' in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([u'@@@@vector',\n",
       "  u'@kata@katja11',\n",
       "  u'@lovex@moi123',\n",
       "  u'@jossu@inkku1',\n",
       "  u'@rm1hevonen',\n",
       "  u'with1891@battle.fi',\n",
       "  u'n@@ranoora1995',\n",
       "  u'nastjanastja@',\n",
       "  u'noor@moimoi',\n",
       "  u'^misu@^sika',\n",
       "  u'esignation1990@battle.fi',\n",
       "  u'from7450@kellotuspaja.comfrom7450@kellotuspaja.com',\n",
       "  u'jones@@joonas12',\n",
       "  u'melu@koira',\n",
       "  u'mir@janna',\n",
       "  u'n@llefrendit66',\n",
       "  u'lucy199@',\n",
       "  u'reki_pukki@hotmail.com123456',\n",
       "  u'sergei!@11sergei11',\n",
       "  u'sergei@11sergei11',\n",
       "  u'temperature8141@battle.fi',\n",
       "  u'tiku.poika@tiku',\n",
       "  u'veer@paska93',\n",
       "  u'\\xe9lisakehela@hotmil.opi123456',\n",
       "  u'pepsigirl@145',\n",
       "  u'1t1h3x8@Lucas',\n",
       "  u'joo@hmm1'],\n",
       " [u'Telewell1\\\\\"', u'turhaa!\"#'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k in allpws if '@' in k],[k for k in allpws if '\\\"' in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'&#9829;perkele', u'jenna;,pumpuli']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k in allpws if ';' in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38395, 38395)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pwlist), len(matchlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'f1leop', u'lapavesi67']\n"
     ]
    }
   ],
   "source": [
    "'''checkprint; two first passwords'''\n",
    "print pwlist[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'leo'], [u'lapa', u'vesi']] lapa [u'f1leop', u'lapavesi67'] lapavesi67 [u'f1leop', u'lapavesi67']\n"
     ]
    }
   ],
   "source": [
    "'''checkprint; two first matchlists, additionally the first value of the second list '''\n",
    "print matchlist[:2], matchlist[1][0], pwlist[:2], pwlist[1], allpws[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YYY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify the Finnish words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the namefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "namelist=[]\n",
    "with open('names.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        name=line.rstrip(\"\\n,\\r\") # remove extra chars\n",
    "        '''\\n - ASCII Linefeed (LF)\n",
    "        \\r - ASCII Carriage Return (CR)'''\n",
    "        namelist.append(name)\n",
    "\n",
    "f.close()\n",
    "\n",
    "namelist=[item.decode('unicode-escape') for item in namelist]\n",
    "namelist=[item.lower() for item in namelist]\n",
    "namelist=[item for item in namelist if len(item)>2] # use only if length is more than 2 chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34024 <type 'unicode'> maria\n"
     ]
    }
   ],
   "source": [
    "print len(namelist), type(namelist[0]), namelist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'ylivirta', u'yl\\xe4-lahti', u'yl\\xe4-rotiala', u'zeus', u'\\xe5kermarck', u'\\xe5kman', u'\\xe4mt\\xf6', u'\\xe4n\\xe4k\\xe4inen', u'\\xf6rth\\xe9n', u'\\xf6ykk\\xf6nen'] [u'maria', u'helena', u'anneli', u'johanna', u'kaarina', u'marjatta', u'hannele', u'kristiina', u'liisa', u'elina']\n"
     ]
    }
   ],
   "source": [
    "print namelist[-10:], namelist[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False\n"
     ]
    }
   ],
   "source": [
    "print u'Tuulia'.lower() in namelist, u'Tuulia' in namelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32834"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nameset = set(namelist)\n",
    "\n",
    "'''add ja to the nameset as that short words were not included in matchsets, but that is very often used in passwords'''\n",
    "nameset.update(['ja']) # 8.10. this would requre changes in main algorithm to put more weight on names and/or frequent words\n",
    "\n",
    "len(nameset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''from a set of substrings, make a list of those which appear as substrings in the target string list'''\n",
    "def strings_in_target(target, string_set, matchlist):    \n",
    "    stringlist = []\n",
    "    for i, tw in enumerate(target): # target word\n",
    "        slist = []\n",
    "        for s in string_set:\n",
    "            ''' 1. string is part of pw, 2. not already in matchlist'''\n",
    "            if s in tw and s not in matchlist[i]:\n",
    "                slist.append(s)\n",
    "        stringlist.append(slist)\n",
    "    return stringlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the function took time  194.910196383  seconds\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "'''Include only names that match the passwords'''\n",
    "start = timeit.default_timer()\n",
    "\n",
    "name_matchlist = strings_in_target(pwlist,nameset, matchlist)\n",
    "\n",
    "stop = timeit.default_timer()    \n",
    "print \"Running the function took time \",stop - start,\" seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38395, 38395, 38395)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(name_matchlist), len(matchlist), len(pwlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combine matchlist and found names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38395"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "word_name_matchlist = [list(itertools.chain(*x)) for x in zip(matchlist,name_matchlist)]\n",
    "len(word_name_matchlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm for the best word combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "Functions:\n",
    "* def checkCombinations\n",
    "* def checkDoubles\n",
    "* def maxN\n",
    "* def findLongestCombination\n",
    "* def mylen\n",
    "* def maybeDoubles\n",
    "* def chooseSameSize\n",
    "* def chooseOneCombinationDict (main function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# All combiword functions together\n",
    "import operator\n",
    "import random\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def checkCombinations(target,target_string, word_list,notfirstone):\n",
    "    '''checks up to 4 combinations from the word_list \n",
    "    which all words together can form the corresponding target word, password'''\n",
    "    combiwords=[]\n",
    "    words=word_list\n",
    "    s=target_string\n",
    "    for i1,w1 in enumerate(words):\n",
    "        weight=(len(w1))\n",
    "        combiwords.append([target, w1,len(w1),(w1)])\n",
    "        if s.find(w1,s.index(w1)+1)!=-1: #checks doubles\n",
    "            combiwords.append([target,(w1+w1),(len(w1)+len(w1)),(w1,w1)])\n",
    "            if s.find(w1+w1,s.index(w1)+1)!=-1: #checks doubles\n",
    "                combiwords.append([target,(w1+w1+w1),(len(w1)+len(w1)+len(w1)),(w1,w1,w1)])\n",
    "        for i2,w2 in enumerate(words):\n",
    "            if s.index(w1)+len(w1)<=s.index(w2) and w1 not in notfirstone:\n",
    "                weight=(len(w1)+len(w2))\n",
    "                combiwords.append([target,(w1+w2),(len(w1)+len(w2)),(w1,w2)])\n",
    "                for i3,w3 in enumerate(words):\n",
    "                    if (s.index(w1)+len(w1)<=s.index(w2) and s.index(w2)+len(w2)<=s.index(w3)):\n",
    "                        combiwords.append([target, (w1+w2+w3),(len(w1)+len(w2)+len(w3)),(w1,w2,w3)])\n",
    "                    for i4,w4 in enumerate(words):\n",
    "                        if (s.index(w1)+len(w1)<=s.index(w2) and s.index(w2)+len(w2)<=s.index(w3) and \n",
    "                            s.index(w3)+len(w3)<=s.index(w4)):\n",
    "                            weight=(len(w1)+len(w2)+len(w3)+len(w4))\n",
    "                            combiwords.append([target, (w1+w2+w3+w4),(len(w1)+len(w2)+len(w3)+len(w4)),(w1,w2,w3,w4)])\n",
    "    return combiwords\n",
    "\n",
    "def checkDoubles(target_string, word_list,origlist):\n",
    "    '''checks if the same word appears twice'''\n",
    "    w1=word_list\n",
    "    s=target_string\n",
    "    if s.find(w1,s.index(w1)+1)!=-1: # checks doubles\n",
    "        return [target_string,(w1+w1),(len(w1)+len(w1)),(origlist+origlist)]\n",
    "    else:\n",
    "        return (target_string, w1, len(w1), origlist)\n",
    "\n",
    "def maxN(datalist):\n",
    "    '''input list of combiwords and weights\n",
    "    will sort first from biggest to smallest \n",
    "    finds max value(s)\n",
    "    return max value, or several, if more than max values'''\n",
    "    if len(datalist)<2: # when only one option exists\n",
    "        return datalist\n",
    "    else:\n",
    "        datalist.sort(key=operator.itemgetter(2), reverse=True)\n",
    "        myindex=1 # help index\n",
    "        for i in datalist:\n",
    "            if datalist[0][2]>datalist[myindex][2]:\n",
    "                return datalist[:myindex] # return all longest ones\n",
    "            else:\n",
    "                myindex=myindex+1\n",
    "                if (myindex==len(datalist)): # if list contains only the longest ones, stop here\n",
    "                    return (datalist[:myindex])\n",
    "\n",
    "def findLongestCombination(target_list, combilist,notfirstone):\n",
    "    '''Finds the longest combination of words'''    \n",
    "    all_combis=[]\n",
    "    for i in xrange(len(combilist)):\n",
    "        target=target_list[i] \n",
    "        starget=target_list[i].lower() # target to lowercase\n",
    "        if combilist[i]!=[] and len(target_list[i].lower())<100:\n",
    "            combis=checkCombinations(target, starget, combilist[i], notfirstone)\n",
    "            all_combis.append(combis)\n",
    "        else: # no comparisons needed for targets with no matches\n",
    "            combis=[[target_list[i],\"\",0,\"\" ]]\n",
    "            all_combis.append(combis)\n",
    "    all_longest=[]\n",
    "    for i in xrange(len(all_combis)):\n",
    "        longest=maxN(all_combis[i])\n",
    "        all_longest.append(longest)\n",
    "    return all_longest\n",
    "\n",
    "def mylen(varorlist):\n",
    "    '''calculates the length of list of matches, \n",
    "    if only one word and therefore not list nor tuple\n",
    "    the length will be 1, which probably is the best match'''\n",
    "    if type(varorlist) in [tuple,list]:\n",
    "        mylen=len(varorlist)\n",
    "    else:\n",
    "        mylen=1\n",
    "    #print mylen\n",
    "    return mylen\n",
    "\n",
    "def maybeDoubles(n):\n",
    "    '''catches like namipupunamipupu combo of four words'''\n",
    "    if n[1] in n[0]:\n",
    "        k=checkDoubles(n[0],n[1],n[3])\n",
    "    else:\n",
    "        k=n\n",
    "    return k\n",
    "\n",
    "\n",
    "def chooseSameSize(n,m, target):\n",
    "    '''checks if combiword has a double like namipupunamipupu and mannepaskamannepaska,\n",
    "    if same size, choose the one that appears first in the password,\n",
    "    n= mylist\n",
    "    m=best '''\n",
    "    if maybeDoubles(n)[2]>n[2]:\n",
    "        k = maybeDoubles(n)\n",
    "    elif maybeDoubles(m)[2]>m[2]:\n",
    "        k = maybeDoubles(m)\n",
    "    else:\n",
    "        if target.find(n[1]) < target.find(m[1]):\n",
    "            k = n\n",
    "        else:\n",
    "            k = m\n",
    "    return k\n",
    "\n",
    "def digitsInString(n): \n",
    "    return len(filter(lambda x: x.isdigit(), str(n)))\n",
    "\n",
    "def leftoverChars(target,item):\n",
    "    newtarget = target.lower() # copy and into \n",
    "    if type(item) not in [list,tuple]:\n",
    "        newtarget = newtarget.replace(item,\"\")\n",
    "    else:\n",
    "        for listitem in item:\n",
    "            newtarget = newtarget.replace(listitem,\"\")\n",
    "    return len(newtarget), digitsInString(newtarget.encode('utf-8')) # 2.10. added .encode('utf-8')\n",
    "\n",
    "'''Check if password starts with the matched string or first string in the matched list'''\n",
    "def pwStartsWith(pw,string):\n",
    "    psw = False\n",
    "    if type(string) not in [list,tuple]:\n",
    "        psw = pw.lower().startswith(string)\n",
    "    else:\n",
    "        psw = pw.lower().startswith(string[0])\n",
    "    return psw\n",
    "\n",
    "'''This is the main function to be called to get a clean dictionary of best hits'''\n",
    "def chooseOneCombinationDict(target_list, combilist,notfirstone):\n",
    "    '''against target list, the combination list is checked, using findLongestCombination,\n",
    "    if more than one longest hit, will choose the shortest list with hits,\n",
    "    which apparently has longest word(s) and therefore best match\n",
    "    AND returns list of chosen matches\n",
    "    this list will be used later for other purpose, but is produced easily here'''\n",
    "    mylist=findLongestCombination(target_list, combilist,notfirstone)\n",
    "    onedict={}\n",
    "    matchlist=[]\n",
    "    for my in xrange(len(mylist)): # len(mylist)\n",
    "        templen=99 # temp var for length of the match\n",
    "        if len(mylist[my])<2: # if only one longest combination\n",
    "            best=(mylist[my])\n",
    "        else: # if more than one longest combination\n",
    "            for ml in mylist[my]:\n",
    "                if templen>mylen(ml[3]): # first word (length=99), or this one has fewer words (is better)\n",
    "                    templen=mylen(ml[3])\n",
    "                    best=ml\n",
    "                elif templen==mylen(ml[3]): # if size is same, calls choseSameSize\n",
    "                    best=chooseSameSize(ml, best, mylist[my][0][0])\n",
    "        if len(best[0][0])==1 and type(best[0]) in [unicode]: # all into same shape, i.e. list of list            \n",
    "            best=[best]            \n",
    "        '''password as key,\n",
    "        best combination, fuzz ration for the best combination'''\n",
    "        leftover, digits = leftoverChars(best[0][0], best[0][3]) # 7.10. , best[0][3] --> , best[0][1]\n",
    "        onedict[best[0][0]] = (best[0][1],fuzz.ratio(best[0][1],best[0][0].lower()),\\\n",
    "                               leftover,digits,pwStartsWith(best[0][0],best[0][3]),len(best[0][3]),\\\n",
    "                               int(np.where(min([len(i) for i in best[0][3]])==1, len(best[0][3]),\\\n",
    "                                            min([len(i) for i in best[0][3]]))),\\\n",
    "                               int(np.where(set('[~!@#$%^&*()_+{}\":;\\']+$-?><=/,.').intersection(best[0][0])>0, \\\n",
    "                                     len(set('[~!@#$%^&*()_+{}\":;\\']+$-?><=/,.').intersection(best[0][0])), 0)),\\\n",
    "                               best[0][3]) #modified 1.10.16 & 4.10.   \n",
    "        matchlist.append(best[0][3])\n",
    "    return onedict, matchlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses the list forms, as that was initially build. Did try to modify it to take also dictionary format, but that was nor possible with feasible attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the function took time  14.5815746022  seconds\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "\n",
    "notfirstone = ['ja']\n",
    "oned, onematch = chooseOneCombinationDict(pwlist,word_name_matchlist, notfirstone)\n",
    "\n",
    "stop = timeit.default_timer()    \n",
    "print \"Running the function took time \",stop - start,\" seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29587\n"
     ]
    }
   ],
   "source": [
    "print len(oned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38395, 19445)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(onematch), len(set(onematch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38395, 19445)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(onematch), len(set(onematch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'iittukaunisto', (u'kaunisto', 76, 5, 0, False, 8, 8, 0, u'kaunisto')),\n",
       " (u'jussilat', (u'jussila', 93, 1, 0, True, 7, 7, 0, u'jussila')),\n",
       " (u'sonja', (u'sonja', 100, 0, 0, True, 5, 5, 0, u'sonja')),\n",
       " (u'kultarakas',\n",
       "  (u'kultarakas', 100, 0, 0, True, 2, 5, 0, (u'kulta', u'rakas'))),\n",
       " (u'aenaenNEANEA', (u'enaenne', 74, 5, 0, False, 2, 3, 0, (u'ena', u'enne'))),\n",
       " (u'petra1', (u'petra', 91, 1, 1, True, 5, 5, 0, u'petra')),\n",
       " (u'protacon', (u'protacon', 100, 0, 0, True, 8, 8, 0, u'protacon')),\n",
       " (u'japizkicoolnews',\n",
       "  (u'coolnews', 70, 7, 0, False, 2, 4, 0, (u'cool', u'news'))),\n",
       " (u'aatami001', (u'aatami', 80, 3, 3, True, 6, 6, 0, u'aatami')),\n",
       " (u'sitruunamarjut',\n",
       "  (u'sitruunamarjut', 100, 0, 0, True, 2, 6, 0, (u'sitruuna', u'marjut'))),\n",
       " (u'estela12', (u'estela', 86, 2, 2, True, 6, 6, 0, u'estela')),\n",
       " (u'yojikkoira', (u'koira', 67, 5, 0, False, 5, 5, 0, u'koira')),\n",
       " (u'camillaviivi',\n",
       "  (u'camillaviivi', 100, 0, 0, True, 2, 5, 0, (u'camilla', u'viivi'))),\n",
       " (u'nalle2', (u'nalle', 91, 1, 1, True, 5, 5, 0, u'nalle')),\n",
       " (u'taavetti', (u'taavetti', 100, 0, 0, True, 8, 8, 0, u'taavetti'))]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''test print of the results'''\n",
    "oned.items()[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'uni', u'kaunis', u'kaunisto']\n",
      "[u'jussi', u'jussila', u'silat']\n",
      "[u'sonja']\n",
      "[u'kulta', u'tara', u'rakas']\n",
      "[u'enne']\n",
      "[u'petra', u'petr']\n",
      "[u'protacon', u'ota', u'pro', u'prota']\n",
      "[u'new', u'cool', u'news']\n",
      "[u'tami', u'aatami']\n",
      "[u'ruuna', u'marjut', u'sitruuna', u'marju', u'sit']\n",
      "[u'tela', u'este']\n",
      "[u'koira']\n",
      "[u'mill', u'camilla', u'milla', u'viivi']\n",
      "[u'nalle', u'all', u'alle']\n",
      "[u'aave', u'taavetti']\n"
     ]
    }
   ],
   "source": [
    "'''test print of the same values, \n",
    "original list of all matches per key values, i.e. pw\n",
    "shows how well the algorithm works, or does not work'''\n",
    "for k in oned.keys()[:15]:\n",
    "    print d0[k.encode('utf-8')] # scandics require the .encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'herneherkku', 92, 2, 2)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oned.values()[15][0], oned.values()[15][1], oned.values()[15][2],oned.values()[15][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before storing result in file, replace @ with AT and , with DOT to ignore transformation problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_oned = pd.DataFrame({'pw' : oned.keys() , 'values' : oned.values() })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pw</th>\n",
       "      <th>values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iittukaunisto</td>\n",
       "      <td>(kaunisto, 76, 5, 0, False, 8, 8, 0, kaunisto)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jussilat</td>\n",
       "      <td>(jussila, 93, 1, 0, True, 7, 7, 0, jussila)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sonja</td>\n",
       "      <td>(sonja, 100, 0, 0, True, 5, 5, 0, sonja)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kultarakas</td>\n",
       "      <td>(kultarakas, 100, 0, 0, True, 2, 5, 0, (kulta,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aenaenNEANEA</td>\n",
       "      <td>(enaenne, 74, 5, 0, False, 2, 3, 0, (ena, enne))</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              pw                                             values\n",
       "0  iittukaunisto     (kaunisto, 76, 5, 0, False, 8, 8, 0, kaunisto)\n",
       "1       jussilat        (jussila, 93, 1, 0, True, 7, 7, 0, jussila)\n",
       "2          sonja           (sonja, 100, 0, 0, True, 5, 5, 0, sonja)\n",
       "3     kultarakas  (kultarakas, 100, 0, 0, True, 2, 5, 0, (kulta,...\n",
       "4   aenaenNEANEA   (enaenne, 74, 5, 0, False, 2, 3, 0, (ena, enne))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_oned.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29587 entries, 0 to 29586\n",
      "Data columns (total 2 columns):\n",
      "pw        29587 non-null object\n",
      "values    29587 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 462.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_oned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_oned['combiword'] = [a for a, b, c, d, e, f, g, h, i in df_oned['values']]\n",
    "df_oned['fuzzy ratio'] = [b for a, b, c, d, e, f, g, h, i in df_oned['values']]\n",
    "df_oned['#leftover chars'] = [c for a, b, c, d, e, f, g, h, i in df_oned['values']]\n",
    "df_oned['#leftover numbers'] = [d for a, b, c, d, e, f, g, h, i in df_oned['values']]\n",
    "df_oned['leftovers are numbers'] = [c==d for a, b, c, d, e, f, g, h, i in df_oned['values']]\n",
    "df_oned['pw starts with match'] = [e for a, b, c, d, e, f, g, h, i in df_oned['values']]\n",
    "df_oned['matched words'] = [i for a, b, c, d, e, f, g, h, i in df_oned['values']]\n",
    "df_oned['min word length'] = [g for a, b, c, d, e, f, g, h, i in df_oned['values']]\n",
    "df_oned['list'] = np.where(df_oned['matched words'].str.contains(\",\"), True, False)\n",
    "length = [f for a, b, c, d, e, f, g, h, i in df_oned['values']]\n",
    "df_oned['#matches'] = np.where(df_oned['list'], length, 1)\n",
    "df_oned['#special chars'] = [h for a, b, c, d, e, f, g, h, i in df_oned['values']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pw</th>\n",
       "      <th>values</th>\n",
       "      <th>combiword</th>\n",
       "      <th>fuzzy ratio</th>\n",
       "      <th>#leftover chars</th>\n",
       "      <th>#leftover numbers</th>\n",
       "      <th>leftovers are numbers</th>\n",
       "      <th>pw starts with match</th>\n",
       "      <th>matched words</th>\n",
       "      <th>min word length</th>\n",
       "      <th>list</th>\n",
       "      <th>#matches</th>\n",
       "      <th>#special chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iittukaunisto</td>\n",
       "      <td>(kaunisto, 76, 5, 0, False, 8, 8, 0, kaunisto)</td>\n",
       "      <td>kaunisto</td>\n",
       "      <td>76</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>kaunisto</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jussilat</td>\n",
       "      <td>(jussila, 93, 1, 0, True, 7, 7, 0, jussila)</td>\n",
       "      <td>jussila</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>jussila</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sonja</td>\n",
       "      <td>(sonja, 100, 0, 0, True, 5, 5, 0, sonja)</td>\n",
       "      <td>sonja</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>sonja</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kultarakas</td>\n",
       "      <td>(kultarakas, 100, 0, 0, True, 2, 5, 0, (kulta,...</td>\n",
       "      <td>kultarakas</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>(kulta, rakas)</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aenaenNEANEA</td>\n",
       "      <td>(enaenne, 74, 5, 0, False, 2, 3, 0, (ena, enne))</td>\n",
       "      <td>enaenne</td>\n",
       "      <td>74</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>(ena, enne)</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>petra1</td>\n",
       "      <td>(petra, 91, 1, 1, True, 5, 5, 0, petra)</td>\n",
       "      <td>petra</td>\n",
       "      <td>91</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>petra</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>protacon</td>\n",
       "      <td>(protacon, 100, 0, 0, True, 8, 8, 0, protacon)</td>\n",
       "      <td>protacon</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>protacon</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>japizkicoolnews</td>\n",
       "      <td>(coolnews, 70, 7, 0, False, 2, 4, 0, (cool, ne...</td>\n",
       "      <td>coolnews</td>\n",
       "      <td>70</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>(cool, news)</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aatami001</td>\n",
       "      <td>(aatami, 80, 3, 3, True, 6, 6, 0, aatami)</td>\n",
       "      <td>aatami</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>aatami</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sitruunamarjut</td>\n",
       "      <td>(sitruunamarjut, 100, 0, 0, True, 2, 6, 0, (si...</td>\n",
       "      <td>sitruunamarjut</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>(sitruuna, marjut)</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                pw                                             values  \\\n",
       "0    iittukaunisto     (kaunisto, 76, 5, 0, False, 8, 8, 0, kaunisto)   \n",
       "1         jussilat        (jussila, 93, 1, 0, True, 7, 7, 0, jussila)   \n",
       "2            sonja           (sonja, 100, 0, 0, True, 5, 5, 0, sonja)   \n",
       "3       kultarakas  (kultarakas, 100, 0, 0, True, 2, 5, 0, (kulta,...   \n",
       "4     aenaenNEANEA   (enaenne, 74, 5, 0, False, 2, 3, 0, (ena, enne))   \n",
       "5           petra1            (petra, 91, 1, 1, True, 5, 5, 0, petra)   \n",
       "6         protacon     (protacon, 100, 0, 0, True, 8, 8, 0, protacon)   \n",
       "7  japizkicoolnews  (coolnews, 70, 7, 0, False, 2, 4, 0, (cool, ne...   \n",
       "8        aatami001          (aatami, 80, 3, 3, True, 6, 6, 0, aatami)   \n",
       "9   sitruunamarjut  (sitruunamarjut, 100, 0, 0, True, 2, 6, 0, (si...   \n",
       "\n",
       "        combiword  fuzzy ratio  #leftover chars  #leftover numbers  \\\n",
       "0        kaunisto           76                5                  0   \n",
       "1         jussila           93                1                  0   \n",
       "2           sonja          100                0                  0   \n",
       "3      kultarakas          100                0                  0   \n",
       "4         enaenne           74                5                  0   \n",
       "5           petra           91                1                  1   \n",
       "6        protacon          100                0                  0   \n",
       "7        coolnews           70                7                  0   \n",
       "8          aatami           80                3                  3   \n",
       "9  sitruunamarjut          100                0                  0   \n",
       "\n",
       "  leftovers are numbers pw starts with match       matched words  \\\n",
       "0                 False                False            kaunisto   \n",
       "1                 False                 True             jussila   \n",
       "2                  True                 True               sonja   \n",
       "3                  True                 True      (kulta, rakas)   \n",
       "4                 False                False         (ena, enne)   \n",
       "5                  True                 True               petra   \n",
       "6                  True                 True            protacon   \n",
       "7                 False                False        (cool, news)   \n",
       "8                  True                 True              aatami   \n",
       "9                  True                 True  (sitruuna, marjut)   \n",
       "\n",
       "   min word length   list  #matches  #special chars  \n",
       "0                8  False         1               0  \n",
       "1                7  False         1               0  \n",
       "2                5  False         1               0  \n",
       "3                5   True         2               0  \n",
       "4                3   True         2               0  \n",
       "5                5  False         1               0  \n",
       "6                8  False         1               0  \n",
       "7                4   True         2               0  \n",
       "8                6  False         1               0  \n",
       "9                6   True         2               0  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_oned.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save best matches as a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''temporary file for csv saving\n",
    "In order to make csv readable, modify special marks:\n",
    "','-->'.' \n",
    "'\\''-->'.' \n",
    "'\\\"'-->'.' \n",
    "';'-->'.'\n",
    "'''\n",
    "\n",
    "df_oned_savecsv = df_oned.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_oned_savecsv['pw'] = np.where(df_oned_savecsv['pw'].str.contains(\",\"), df_oned_savecsv['pw'].str.replace(\",\", \".\"), df_oned_savecsv['pw'])\n",
    "df_oned_savecsv['pw'] = np.where(df_oned_savecsv['pw'].str.contains(\"\\'\"), df_oned_savecsv['pw'].str.replace(\"\\'\", \".\"), df_oned_savecsv['pw'])\n",
    "df_oned_savecsv['pw'] = np.where(df_oned_savecsv['pw'].str.contains(\"\\\"\"), df_oned_savecsv['pw'].str.replace(\"\\\"\", \".\"), df_oned_savecsv['pw'])\n",
    "df_oned_savecsv['pw'] = np.where(df_oned_savecsv['pw'].str.contains(\";\"), df_oned_savecsv['pw'].str.replace(\";\", \".\"), df_oned_savecsv['pw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pw</th>\n",
       "      <th>values</th>\n",
       "      <th>combiword</th>\n",
       "      <th>fuzzy ratio</th>\n",
       "      <th>#leftover chars</th>\n",
       "      <th>#leftover numbers</th>\n",
       "      <th>leftovers are numbers</th>\n",
       "      <th>pw starts with match</th>\n",
       "      <th>matched words</th>\n",
       "      <th>min word length</th>\n",
       "      <th>list</th>\n",
       "      <th>#matches</th>\n",
       "      <th>#special chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iittukaunisto</td>\n",
       "      <td>(kaunisto, 76, 5, 0, False, 8, 8, 0, kaunisto)</td>\n",
       "      <td>kaunisto</td>\n",
       "      <td>76</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>kaunisto</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jussilat</td>\n",
       "      <td>(jussila, 93, 1, 0, True, 7, 7, 0, jussila)</td>\n",
       "      <td>jussila</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>jussila</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              pw                                          values combiword  \\\n",
       "0  iittukaunisto  (kaunisto, 76, 5, 0, False, 8, 8, 0, kaunisto)  kaunisto   \n",
       "1       jussilat     (jussila, 93, 1, 0, True, 7, 7, 0, jussila)   jussila   \n",
       "\n",
       "   fuzzy ratio  #leftover chars  #leftover numbers leftovers are numbers  \\\n",
       "0           76                5                  0                 False   \n",
       "1           93                1                  0                 False   \n",
       "\n",
       "  pw starts with match matched words  min word length   list  #matches  \\\n",
       "0                False      kaunisto                8  False         1   \n",
       "1                 True       jussila                7  False         1   \n",
       "\n",
       "   #special chars  \n",
       "0               0  \n",
       "1               0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_oned_savecsv.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_oned_savecsv = df_oned_savecsv.drop(['values'], axis=1); # without semicolon the dataset would be printed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pw</th>\n",
       "      <th>combiword</th>\n",
       "      <th>fuzzy ratio</th>\n",
       "      <th>#leftover chars</th>\n",
       "      <th>#leftover numbers</th>\n",
       "      <th>leftovers are numbers</th>\n",
       "      <th>pw starts with match</th>\n",
       "      <th>matched words</th>\n",
       "      <th>min word length</th>\n",
       "      <th>list</th>\n",
       "      <th>#matches</th>\n",
       "      <th>#special chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iittukaunisto</td>\n",
       "      <td>kaunisto</td>\n",
       "      <td>76</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>kaunisto</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jussilat</td>\n",
       "      <td>jussila</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>jussila</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              pw combiword  fuzzy ratio  #leftover chars  #leftover numbers  \\\n",
       "0  iittukaunisto  kaunisto           76                5                  0   \n",
       "1       jussilat   jussila           93                1                  0   \n",
       "\n",
       "  leftovers are numbers pw starts with match matched words  min word length  \\\n",
       "0                 False                False      kaunisto                8   \n",
       "1                 False                 True       jussila                7   \n",
       "\n",
       "    list  #matches  #special chars  \n",
       "0  False         1               0  \n",
       "1  False         1               0  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_oned_savecsv.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = \"tko_2027/data/\"+sourcename+\"_before_validation_pwsandmatches.csv\"\n",
    "df_oned_savecsv.to_csv(filename, sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df_oned_savecsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Remove extra column'''\n",
    "df_oned = df_oned.drop(['values'], axis=1); # without semicolon the dataset would be printed below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter/select correct matches\n",
    "NOTE! These need to be in this exact order!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_temp = df_oned.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29587, 29587)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_temp), len(df_oned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_valid = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17286"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''All valid matches:\n",
    "* Leftovers are numbers TRUE'''\n",
    "df_valid = df_temp[df_temp['leftovers are numbers']]\n",
    "len(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12301, 12301)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp = df_temp[~(df_temp['leftovers are numbers'])]\n",
    "len(df_temp), len(df_oned)-len(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_new = pd.concat([df_a, df_b])\n",
    "# df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''All valid:\n",
    "* leftovers are numbers FALSE\n",
    "* PW start FALSE\n",
    "* leftover chars == 2\n",
    "* first chars are in spec_list'''\n",
    "\n",
    "spec_list = ['is','in','en']\n",
    "\n",
    "mytemp = df_temp[(~(df_temp['leftovers are numbers'])) & (~(df_temp['pw starts with match'])) & \\\n",
    "        (df_temp['#leftover chars']==2) & (df_temp['pw'].str[0:2].isin(spec_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17298"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid = pd.concat([df_valid, mytemp])\n",
    "len(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12289, 12289)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''remove same data as complement from df_temp'''\n",
    "df_temp = df_temp[~((~(df_temp['leftovers are numbers'])) & (~(df_temp['pw starts with match'])) & \\\n",
    "        (df_temp['#leftover chars']==2) & (df_temp['pw'].str[0:2].isin(spec_list)))]\n",
    "len(df_temp), len(df_oned)-len(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''All valid:\n",
    "* fuzzy ratio == 100\n",
    "* min word length >=4'''\n",
    "\n",
    "mytemp = df_temp[(df_temp['fuzzy ratio']==100) & (df_temp['min word length']>=4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17309"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid = pd.concat([df_valid, mytemp])\n",
    "len(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12278, 12278)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''remove same data as complement from df_temp'''\n",
    "df_temp = df_temp[~((df_temp['fuzzy ratio']==100) & (df_temp['min word length']>=4))]\n",
    "len(df_temp), len(df_oned)-len(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''NOT valid:\n",
    "* PW start TRUE\n",
    "* min word length ==3'''\n",
    "\n",
    "df_drop = df_temp[(df_temp['pw starts with match']) & (df_temp['min word length']==3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1520"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10758, 10758)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''remove same data as complement from df_temp'''\n",
    "df_temp = df_temp[~((df_temp['pw starts with match']) & (df_temp['min word length']==3))]\n",
    "len(df_temp), len(df_oned)-len(df_valid)-len(df_drop)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4530"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''All valid:\n",
    "* PW start TRUE\n",
    "* min word length > 3'''\n",
    "\n",
    "mytemp = df_temp[(df_temp['pw starts with match']) & (df_temp['min word length']>3)]\n",
    "len(mytemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21839"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid = pd.concat([df_valid, mytemp])\n",
    "len(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6228, 7748)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''remove same data as complement from df_temp'''\n",
    "df_temp = df_temp[~((df_temp['pw starts with match']) & (df_temp['min word length']>3))]\n",
    "len(df_temp), len(df_oned)-len(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "402"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''All valid:\n",
    "* fuzzy ratio >=85\n",
    "* list FALSE'''\n",
    "\n",
    "mytemp = df_temp[(~(df_temp['list'])) & (df_temp['fuzzy ratio']>=85)]\n",
    "len(mytemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22241"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid = pd.concat([df_valid, mytemp])\n",
    "len(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1132, 7346)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''remove same data as complement from df_temp'''\n",
    "df_temp = df_temp[(~(~(df_temp['list'])) & (df_temp['fuzzy ratio']>=85))]\n",
    "len(df_temp), len(df_oned)-len(df_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store valid matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filename = \"tko_2027/data/\"+sourcename+\"_valid_file_content.csv\"\n",
    "# df_valid.to_csv(filename, sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all the valid words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Itearative function for flattening the list of strings and lists as one list\n",
    "* if item is a tuple; call function iteratively\n",
    "* else item is string; add to new list'''\n",
    "def flatten(mylist):\n",
    "    newlist = []\n",
    "    for item in mylist:\n",
    "        if isinstance(item, tuple) or isinstance(item, list):\n",
    "            newlist.extend(flatten(item))            \n",
    "        else:\n",
    "            newlist.append(item)\n",
    "    return newlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22241, 29553)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_pws = flatten(df_valid['pw'])\n",
    "selected_words = flatten(df_valid['matched words'])\n",
    "len(selected_pws),len(selected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordsum_dict={}\n",
    "for selword in set(selected_words):\n",
    "    wordsum_dict[selword]=0\n",
    "    for i in selected_pws:\n",
    "        if (type(oned[i][-1])==tuple and selword in oned[i][-1]) or (type(oned[i][-1])==unicode and selword == oned[i][-1]):\n",
    "            wordsum_dict[selword] = wordsum_dict[selword]+allpw_counter[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11749, 29553, 11749)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordsum_dict), len(selected_words), len(set(selected_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'koira', 72),\n",
       " (u'man', 70),\n",
       " (u'kakka', 67),\n",
       " (u'paska', 65),\n",
       " (u'love', 57),\n",
       " (u'kissa', 56),\n",
       " (u'ville', 55),\n",
       " (u'sami', 52),\n",
       " (u'make', 51),\n",
       " (u'mikko', 51)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from collections import Counter\n",
    "\n",
    "'''count all occurrences of unique words in the selected words (to be used later)\n",
    "print out 10 most common ones'''\n",
    "word_pwfreq = Counter(selected_words)\n",
    "pw_counter = Counter(pwlist)\n",
    "#allpw_counter = Counter(allpws) # done earlier already\n",
    "Counter(selected_words).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11749, 11749, 38395, 29587, 38395, 29587)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_pwfreq), len(set(selected_words)), len(pwlist), len(pw_counter), len(allpws), len(allpw_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'ari', 1024),\n",
       " (u'ala', 884),\n",
       " (u'eri', 790),\n",
       " (u'all', 785),\n",
       " (u'per', 710),\n",
       " (u'san', 706),\n",
       " (u'one', 575),\n",
       " (u'min', 508),\n",
       " (u'las', 495),\n",
       " (u'sam', 431)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''the most common ones before validation'''\n",
    "Counter(flatten(matchlist)).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify the Finnish words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the needed words from morphological analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File filtered_vocab_omor.gz, located on a Linux server, contains morphological analysis. This ready made analysis file contains Finnish language morphological analysis, so this file can be utilized to identify Finnish words in the password matchlists.\n",
    "Below a short desciption of the query tools used to get the needed information and file from utu Linux server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting uniq Finnish words from filtered_vocab_omor.gz and storing the result into filtered_vocab_omor_uniq.gz\n",
    "\n",
    "Example lines from filtered_vocab_omor.gz (inluces empty lines and non-Finnish words that end with +?):\n",
    "<blockquote>\n",
    "        yhteisöllisin   yhteisö<N><Der_llinen><A><Pl><Ins><cap> 0.0<br>\n",
    "        yhteisöllisin   yhteisöllinen<A><Pos><Pl><Ins><cap>     0.0<br>\n",
    "        yhteisöllisin   yhteisöllinen<A><Superl><Sg><Nom><cap>  0.0<<br>\n",
    "        viewpoint       +?<br>\n",
    "        supercrossiin   +?<br>\n",
    "        tovoitteet      +?<br>\n",
    "</blockquote>     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get unique words, need to\n",
    "* grep lines that start with words, as the file contains also empty lines\n",
    "* get only the first column\n",
    "* sort\n",
    "* get unique words and their count\n",
    "* sort in reverse order (biggest frequency first)\n",
    "* for visual inspection, only 10 first are needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~~~~~~~~~~~~~~~~~~\n",
    "$ zcat filtered_vocab_omor.gz | grep -P '^[a-zåäöA-ZÅÄÖ]' | cut -f 1 | sort | uniq -c | sort -nr | head<br></code>\n",
    "~~~~~~~~~~~~~~~~~~~~\n",
    "<blockquote>\n",
    "        1152 isoisoisoisoisoisoisovanhempani\n",
    "        512 häähäähäähäähäähäähäähäähää\n",
    "        432 isoisoisoisoisoisoäitini\n",
    "        384 iso-iso-iso-iso-iso-iso-iso-vanhempien\n",
    "        216 isoisovanhempansa\n",
    "        216 isoisoisoisoisovanhemmat\n",
    "        216 isoisoisoisänsä\n",
    "        216 isoisoisoäitinsä\n",
    "        192 toimittaja-kirjailija-lääketiedekirjoittaja-kääntäjä-copywriter-valokuvaaja\n",
    "        180 isovanhempansa\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one however included also non-Finnish words.\n",
    "\n",
    "Filtering first columns of lines that include '0.0', as all those those lines start with Finnish words:\n",
    "\n",
    "~~~~~~~~~~~~~~~~~~~~\n",
    "$ zcat filtered_vocab_omor.gz | grep -P '0.0' | cut -f 1 | sort | uniq | head\n",
    "~~~~~~~~~~~~~~~~~~~~\n",
    "<blockquote>        \n",
    "        a-aalto\n",
    "        a-äänellä\n",
    "        a-äänestysalueen\n",
    "        a-äänet\n",
    "        a-ääni\n",
    "        a-äänistä\n",
    "        a-äänne\n",
    "        ä-äänne\n",
    "        a-äännettä\n",
    "        a-äänteeksi\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the query to a zip file for further usage:\n",
    "\n",
    "~~~~~~~~~~~~~~~~~~~~\n",
    "zcat filtered_vocab_omor.gz | grep -P '0.0' | cut -f 1 | sort | uniq | gzip > filtered_vocab_omor_uniq.gz\n",
    "~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "Just out of curiosity, how many lines in the original file?\n",
    "\n",
    "~~~~~~~~~~~~~~~~~~~~\n",
    "$ zcat filtered_vocab_omor.gz | wc -l\n",
    "~~~~~~~~~~~~~~~~~~~~\n",
    "<blockquote>\n",
    "    31962465\n",
    "</blockquote>\n",
    "\n",
    "How many non-empty lines in the original file?\n",
    "\n",
    "~~~~~~~~~~~~~~~~~~~~\n",
    "$ zcat filtered_vocab_omor.gz | grep -v \"^[[:space:]]*$\" | wc -l\n",
    "~~~~~~~~~~~~~~~~~~~~\n",
    "<blockquote>\n",
    "    20227331\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of lines in the new file (one word per line, nothing else):\n",
    "\n",
    "~~~~~~~~~~~~~~~~~~~~\n",
    "$ zcat filtered_vocab_omor_uniq.gz | wc -l\n",
    "~~~~~~~~~~~~~~~~~~~~\n",
    "<blockquote>\n",
    "    6924238 \n",
    "</blockquote>\n",
    "So out of 20 million words the number of unique Finnish words is 6,9 million.\n",
    "\n",
    "Copying from linux server to win computer:\n",
    "\n",
    "~~~~~~~~~~~~~~~~~~~~\n",
    "pscp -l elmasyr TY1******.utu.fi:/home/elmasyr/password_analyzer/filtered_vocab_omor.gz C:\\mytemp\n",
    "~~~~~~~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to now check the found words vs. filtered_vocab_omor_uniq.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading omor_uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readgz_singlecol_tolist(filename):\n",
    "    '''reads gz file that has words in first column,\n",
    "    no other columns in the file,\n",
    "    returns file content as list in utf-8 format'''\n",
    "    fwrds=[]\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        for line in f:\n",
    "            word =line.split()\n",
    "            fwrds.append(word) #word could be replaced with i.decode('utf-8') for i in word, \n",
    "                                #but seemmed to take long time, so below the fix\n",
    "\n",
    "    f.close()\n",
    "    #wordlist to unicode\n",
    "    finnwords=[x[0].decode('utf-8') for x in fwrds]\n",
    "    #print len(fwrds), len(finnwords) # check\n",
    "    return finnwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the function took time  52.1509844257  seconds\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "finnwords = readgz_singlecol_tolist('filtered_vocab_omor_uniq.gz')\n",
    "\n",
    "stop = timeit.default_timer()    \n",
    "print \"Running the function took time \",stop - start,\" seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'a-aalto', u'a-\\xe4\\xe4nell\\xe4', u'a-\\xe4\\xe4nestysalueen', u'a-\\xe4\\xe4net', u'a-\\xe4\\xe4ni']\n"
     ]
    }
   ],
   "source": [
    "print [i for i in finnwords[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a-aalto\n",
      "a-äänellä\n",
      "a-äänestysalueen\n",
      "a-äänet\n",
      "a-ääni\n",
      "a-äänistä\n",
      "a-äänne\n",
      "ä-äänne\n",
      "a-äännettä\n",
      "a-äänteeksi\n"
     ]
    }
   ],
   "source": [
    "for i in finnwords[:10]:    \n",
    "    print i\n",
    "    #i[0].encode(\"utf-8\") #decode needed in order to print to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'a-aalto', u'a-\\xe4\\xe4nell\\xe4', u'a-\\xe4\\xe4nestysalueen', u'a-\\xe4\\xe4net', u'a-\\xe4\\xe4ni']\n"
     ]
    }
   ],
   "source": [
    "print finnwords[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Matches vs. omor_uniq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note! the list of unique passwords in this testing round (using matches_round1.gz) is 43870, but the number of unique matches is only 15148 (!).\n",
    "\n",
    "When looking this further, notices that most of the matches are empty lists, i.e. [] appears 16650 times (xxx need to check this as the value deviates from the check part below), word \"akka\" appears 390 times, \"you\" 68 times etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The function and call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def finnwords_tolist(list_x,list_y):\n",
    "    '''get dictionary (with pw as keys, words as values)\n",
    "    and list of words,\n",
    "    return intersection of the values and words as list'''\n",
    "    s1=set(list_x)\n",
    "    s2=set(finnwords)\n",
    "    s3 = s1 & s2\n",
    "    return list(s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4937"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finnwlist = finnwords_tolist(selected_words, finnwords)\n",
    "len(finnwlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<code>\n",
    "$ zcat all_vocab.gz | wc -l\n",
    "28399265\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Importing all_vocab to dictionary format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For the following function, need too decide if all macthes need to be stored.\n",
    "Currently stores the last occurrence.\n",
    "\n",
    "<blockquote>\n",
    "        $ zcat all_vocab | grep -P '^yellow\\t'\n",
    "        yellow  PROPN   _       1233\n",
    "        yellow  NOUN    Case=Par|Number=Sing    2\n",
    "        yellow  NOUN    Case=Nom|Number=Sing    138\n",
    "        yellow  ADV     _       40\n",
    "        yellow  SYM     _       27\n",
    "        yellow  X       Foreign=Foreign 350\n",
    "        yellow  ADJ     Case=Nom|Degree=Pos|Number=Sing 14\n",
    "        yellow  PROPN   Case=Nom|Number=Sing    862\n",
    "        yellow  NOUN    _       2\n",
    "        yellow  NUM     Case=Nom|Number=Sing|NumType=Card       2\n",
    "        yellow  INTJ    _       4\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get frequency and related morphological analysis\n",
    "Note! Stores only the last occurrence of the word and related values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_frequencies(filename, lookup_set):\n",
    "#     '''get web crawled data in gz with tab delimiter (word, pos, analysis, freq)\n",
    "#     all the first column words that match the lookup set content\n",
    "#     will be stored in a dictionary, including frequency and other values\n",
    "#     '''\n",
    "#     dw={}\n",
    "#     n=0 # will be removed later\n",
    "#     with gzip.open(filename, 'rb') as f:    \n",
    "#         for line in f:        \n",
    "#             line=line.rstrip(\"\\n\") # removes the line end mark\n",
    "#             word, pos, analysis, freq=line.split(\"\\t\")        \n",
    "#             word=word.decode('utf-8') # convert word to unicode\n",
    "#             '''additional check that the function proceeds\n",
    "#             will remove this and all related in later versions''' \n",
    "#             n+=1\n",
    "#             if n%1000000 == 0:\n",
    "#                 ntimer=timeit.default_timer()\n",
    "#                 print n, ntimer-start\n",
    "#             if word in lookup_set: # using set s1 here, as set is much faster than list (matched)\n",
    "#                 dw[word]=(int(freq), pos, analysis)# pos, analysis\n",
    "#                 '''note! currently stores the last occurrence of the \n",
    "#                 word and related values\n",
    "#                 -- needs tuning'''       \n",
    "#     f.close()\n",
    "#     return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # zcat all_vocab.gz | wc -l\n",
    "# # 28399265 - the amount of lines in the file\n",
    "\n",
    "# start = timeit.default_timer()\n",
    "\n",
    "# dw = get_frequencies('all_vocab.gz', set(selected_words))\n",
    "\n",
    "# stop = timeit.default_timer()    \n",
    "# print \"Running the function took time \",stop - start,\" seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get sum of frequencies per word\n",
    "Only stores the frequency of the word.\n",
    "If the word has several different morhological results, a cumulative frequency is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_freq_sums(filename, lookup_set):\n",
    "    '''get web crawled data in gz with tab delimiter (word, pos, analysis, freq)\n",
    "    all the first column words that match the lookup set content\n",
    "    will be stored in a dictionary, including frequency and other values\n",
    "    '''\n",
    "    dw={}\n",
    "    n=0 # will be removed later\n",
    "    with gzip.open(filename, 'rb') as f:    \n",
    "        for line in f:        \n",
    "            line=line.rstrip(\"\\n\") # removes the line end mark\n",
    "            word, freq=line.split(\"\\t\")        \n",
    "            word=word.decode('utf-8') # convert word to unicode\n",
    "            freqn=int(freq)\n",
    "            '''additional check that the function proceeds\n",
    "            will remove this and all related in later versions''' \n",
    "            n+=1\n",
    "            if n%1000000 == 0:\n",
    "                ntimer=timeit.default_timer()\n",
    "                print n, ntimer-start\n",
    "            '''using set s1 here, as set is much faster than list (matched)'''\n",
    "            if word in lookup_set: \n",
    "                if word in dw.keys():\n",
    "                    dw[word]=dw[word]+freqn\n",
    "                else:\n",
    "                    dw[word]=freqn\n",
    "                '''stores the sum of frequencies'''       \n",
    "    f.close()\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 6.67347181963\n",
      "2000000 13.3870037106\n",
      "3000000 20.2917551354\n",
      "4000000 27.3386040152\n",
      "5000000 34.5794901238\n",
      "6000000 41.8180762363\n",
      "7000000 49.107537077\n",
      "8000000 56.4584199015\n",
      "9000000 63.9484268346\n",
      "10000000 71.3478912331\n",
      "11000000 79.1019963328\n",
      "12000000 86.7148666185\n",
      "13000000 94.1976451552\n",
      "14000000 101.691108354\n",
      "15000000 109.284726863\n",
      "16000000 116.891177799\n",
      "17000000 124.520095603\n",
      "18000000 132.174340162\n",
      "19000000 139.853485002\n",
      "20000000 147.502804581\n",
      "21000000 155.035435562\n",
      "22000000 162.654502838\n",
      "23000000 170.561364398\n",
      "24000000 178.841007779\n",
      "25000000 187.345284495\n",
      "26000000 195.274665377\n",
      "27000000 203.171802412\n",
      "28000000 211.301358218\n",
      "Running the function took time  214.353535745  seconds\n"
     ]
    }
   ],
   "source": [
    "# zcat all_vocab.gz | wc -l\n",
    "# 28399265 - the amount of lines in the file\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "dw_freqsum = get_freq_sums('all_vocab_freq.gz', set(selected_words))\n",
    "\n",
    "stop = timeit.default_timer()    \n",
    "print \"Running the function took time \",stop - start,\" seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get morphological analysis\n",
    "Morphological analysis is chosen by the biggest frequncy of the used analysis data; assumed that the most used wrod and it's analysis would be most suitable for the word.\n",
    "In most cases this seems to go right, but due to the nature of words used in passwords e.g. \"anna\" is usually a proper noun (name) in passwords, but in web data more ofter verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_omorph_analysis(filename, lookup_set):\n",
    "    '''get web crawled data in gz with tab delimiter (word, pos, analysis, freq)\n",
    "    all the first column words that match the lookup set content\n",
    "    will be stored in a dictionary, including frequency and other values\n",
    "    '''\n",
    "    dw={}\n",
    "    n=0 # will be removed later\n",
    "    with gzip.open(filename, 'rb') as f:    \n",
    "        for line in f:        \n",
    "            line=line.rstrip(\"\\n\") # removes the line end mark\n",
    "            word, pos, analysis, freq=line.split(\"\\t\")               \n",
    "            word=word.decode('utf-8') # convert word to unicode\n",
    "            freqn=int(freq)\n",
    "            '''additional check that the function proceeds\n",
    "            will remove this and all related in later versions''' \n",
    "            n+=1\n",
    "            if n%1000000 == 0:\n",
    "                ntimer=timeit.default_timer()\n",
    "                print n, ntimer-start\n",
    "            '''using set here, as set is much faster than list (matched)'''\n",
    "            if word in lookup_set: \n",
    "                if word in dw.keys():                    \n",
    "#                     print word, dw[word][0]\n",
    "#                     print freq\n",
    "                    if dw[word][0]>int(freq):\n",
    "#                        print 'vanha parempi'\n",
    "                        continue\n",
    "                    else:\n",
    "#                        print 'uusi arvo', freq\n",
    "                        dw[word]=(int(freq), pos, analysis)\n",
    "                else:\n",
    "#                    print word\n",
    "                    dw[word]=(int(freq), pos, analysis)\n",
    "                '''stores values for the bigger frequency'''       \n",
    "    f.close()\n",
    "    return dw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 7.1340394865\n",
      "2000000 14.4941582151\n",
      "3000000 22.8838650198\n",
      "4000000 30.7591492409\n",
      "5000000 40.1492808093\n",
      "6000000 49.4504280707\n",
      "7000000 58.3873201453\n",
      "8000000 66.4626236376\n",
      "9000000 74.5755378251\n",
      "10000000 83.5707565243\n",
      "11000000 91.9726189852\n",
      "12000000 100.130294723\n",
      "13000000 110.025101544\n",
      "14000000 118.442906046\n",
      "15000000 127.031091569\n",
      "16000000 135.645868445\n",
      "17000000 144.208971809\n",
      "18000000 153.00332228\n",
      "19000000 161.279544744\n",
      "20000000 170.747271249\n",
      "21000000 179.136584648\n",
      "22000000 187.206251041\n",
      "23000000 195.318195968\n",
      "24000000 203.457190652\n",
      "25000000 211.577474634\n",
      "26000000 219.743871428\n",
      "27000000 227.89405821\n",
      "28000000 235.887087523\n",
      "Running the function took time  239.136960949  seconds\n"
     ]
    }
   ],
   "source": [
    "# zcat all_vocab.gz | wc -l\n",
    "# 28399265 - the amount of lines in the file\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "dw_omor = get_omorph_analysis('all_vocab.gz', set(selected_words))\n",
    "\n",
    "stop = timeit.default_timer()    \n",
    "print \"Running the function took time \",stop - start,\" seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11742\n"
     ]
    }
   ],
   "source": [
    "print len(dw_freqsum)#, dw_freqsum['yellow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'toisi', 16427),\n",
       " (u'toise', 198),\n",
       " (u'yellow', 2674),\n",
       " (u'factory', 3074),\n",
       " (u'four', 5076),\n",
       " (u'woods', 4603),\n",
       " (u'jihaa', 272),\n",
       " (u'mirage', 556),\n",
       " (u'woody', 2090),\n",
       " (u'suzana', 18)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw_freqsum.items()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11742\n"
     ]
    }
   ],
   "source": [
    "print len(dw_omor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'toisi',\n",
       "  (15822, 'VERB', 'Mood=Cnd|Number=Sing|Person=3|VerbForm=Fin|Voice=Act')),\n",
       " (u'toise', (114, 'ADV', '_')),\n",
       " (u'yellow', (1233, 'PROPN', '_')),\n",
       " (u'factory', (1954, 'PROPN', 'Case=Nom|Number=Sing')),\n",
       " (u'four', (2442, 'PROPN', '_')),\n",
       " (u'woods', (4020, 'PROPN', 'Case=Nom|Number=Sing')),\n",
       " (u'jihaa', (221, 'NOUN', 'Case=Par|Number=Sing')),\n",
       " (u'mirage', (396, 'PROPN', 'Case=Nom|Number=Sing')),\n",
       " (u'woody', (1534, 'PROPN', '_')),\n",
       " (u'suzana', (8, 'PROPN', '_'))]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw_omor.items()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine matched words related information in one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29553"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'unicode'> <type 'unicode'>\n",
      "13782\n"
     ]
    }
   ],
   "source": [
    "'''Related to the names, \n",
    "a) checking that the types are identical\n",
    "b) Just out of curiosity, how many names in the matchset?\n",
    "'''\n",
    "print type(namelist[0]), type(selected_words[0])\n",
    "print len([item for item in selected_words if item in namelist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## match_freq_finn_names\n",
    "\n",
    "Features:\n",
    "* The found match\n",
    "* Frequency the match has been used in these passwords\n",
    "* Sum of frequencies, match has been used in web text\n",
    "* Is this a Finnish word?\n",
    "* Is this a Finnish name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11749, 11742, 7, 29553)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Names do not necessarily have morphological analysis, so those are separated\n",
    "wordset = set(selected_words)\n",
    "wordset1 = [item for item in wordset if item in dw_omor.keys()]\n",
    "wordset2 = [item for item in wordset if item not in dw_omor.keys()]\n",
    "len(wordset),len(wordset1),len(wordset2), len(selected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11749 11742 7\n",
      "2045.41043756\n"
     ]
    }
   ],
   "source": [
    "# Combining all needed details for storing the data\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "match_freq_finn_names2=[(item, wordsum_dict[item], 0, \\\n",
    "                        0, 'PROPN', 'Case=Nom|Number=Sing',\\\n",
    "                        item in finnwords, item in namelist) for item in wordset2]\n",
    "\n",
    "\n",
    "match_freq_finn_names1=[(item, wordsum_dict[item], dw_freqsum[item], \\\n",
    "                        dw_omor[item][0], dw_omor[item][1], dw_omor[item][2],\\\n",
    "                        item in finnwords, item in namelist) for item in wordset1]\n",
    "\n",
    "match_freq_finn_names = match_freq_finn_names1 + match_freq_finn_names2\n",
    "\n",
    "print len(match_freq_finn_names), len(match_freq_finn_names1), len(match_freq_finn_names2)\n",
    "\n",
    "stop=timeit.default_timer()\n",
    "print stop-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'toisi',\n",
       "  2,\n",
       "  16427,\n",
       "  15822,\n",
       "  'VERB',\n",
       "  'Mood=Cnd|Number=Sing|Person=3|VerbForm=Fin|Voice=Act',\n",
       "  True,\n",
       "  False),\n",
       " (u'toise', 1, 198, 114, 'ADV', '_', False, False),\n",
       " (u'sonja', 16, 8760, 8530, 'PROPN', 'Case=Nom|Number=Sing', False, True),\n",
       " (u'factory', 1, 3074, 1954, 'PROPN', 'Case=Nom|Number=Sing', False, False),\n",
       " (u'four', 3, 5076, 2442, 'PROPN', '_', False, False),\n",
       " (u'woods', 1, 4603, 4020, 'PROPN', 'Case=Nom|Number=Sing', False, True),\n",
       " (u'jihaa', 3, 272, 221, 'NOUN', 'Case=Par|Number=Sing', False, False),\n",
       " (u'mirage', 3, 556, 396, 'PROPN', 'Case=Nom|Number=Sing', False, False),\n",
       " (u'woody', 2, 2090, 1534, 'PROPN', '_', False, False),\n",
       " (u'suzana', 1, 18, 8, 'PROPN', '_', False, True)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_freq_finn_names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DW\n",
    "\n",
    "[(u'myy', 4, 50973, (2, 'SCONJ', '_'), True, True),\n",
    " (u'luuranko', 1, 1867, (26, 'PROPN', 'Case=Nom|Number=Sing'), True, False),\n",
    " (u'devils', 1, 1678, (1120, 'PROPN', 'Case=Nom|Number=Sing'), False, False),\n",
    " (u'rinkula', 1, 415, (6, 'PROPN', 'Case=Nom|Number=Sing'), True, False),\n",
    " (u'sonja', 2, 8760, (2, 'NOUN', 'Case=Par|Number=Plur'), False, True),\n",
    " (u'topikissa', 1, 373, (354, 'NOUN', 'Case=Ine|Number=Sing'), False, False),\n",
    " (u'pikku',9, 92129, (231, 'ADJ', 'Case=Nom|Degree=Pos|Number=Sing'),True, False),\n",
    " (u'unelma', 2, 24240, (1411, 'PROPN', 'Case=Nom|Number=Sing'), True, True),\n",
    " (u'iida', 1, 9383, (90, 'NOUN', 'Case=Nom|Number=Sing'), False, True),\n",
    " (u'pulse', 1, 714, (75, 'NOUN', 'Case=Nom|Number=Sing'), False, False)]\n",
    " \n",
    "DW_omor\n",
    "\n",
    "[(u'myy',4, 50973, (39400,'VERB','Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act'),True,True),\n",
    " (u'luuranko', 1, 1867, (1833, 'NOUN', 'Case=Nom|Number=Sing'), True, False),\n",
    " (u'devils', 1, 1678, (1120, 'PROPN', 'Case=Nom|Number=Sing'), False, False),\n",
    " (u'rinkula', 1, 415, (409, 'NOUN', 'Case=Nom|Number=Sing'), True, False),\n",
    " (u'sonja', 2, 8760, (8530, 'PROPN', 'Case=Nom|Number=Sing'), False, True),\n",
    " (u'topikissa', 1, 373, (354, 'NOUN', 'Case=Ine|Number=Sing'), False, False),\n",
    " (u'pikku', 9, 92129, (80639, 'ADV', '_'), True, False),\n",
    " (u'unelma', 2, 24240, (22829, 'NOUN', 'Case=Nom|Number=Sing'), True, True),\n",
    " (u'iida', 1, 9383, (9248, 'PROPN', 'Case=Nom|Number=Sing'), False, True),\n",
    " (u'pulse', 1, 714, (367, 'PROPN', 'Case=Nom|Number=Sing'), False, False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save match_freq_finn_names as a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tko_2027/data/78k_matchfreqfinnw_names.csv\n"
     ]
    }
   ],
   "source": [
    "filename = \"tko_2027/data/\"+sourcename+\"_matchfreqfinnw_names.csv\"\n",
    "print filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfmatch_names=pd.DataFrame(match_freq_finn_names, columns = [\"matched word\", \"freq in pws\", \"freq in web\",\\\n",
    "                                                             \"individual freq\", \"POS\", \"analysis\", \"finnish word\", \"finnish name\"])\n",
    "filename = \"tko_2027/data/\"+sourcename+\"_matchfreqfinnw_names.csv\"\n",
    "dfmatch_names.to_csv(filename, sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38395, 29587, 7346, 22241, 22241, 29587)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwset = set(allpws)\n",
    "pwset1 = [item for item in pwset if item not in selected_pws]\n",
    "pwset2 = [item for item in pwset if item in selected_pws]\n",
    "len(allpws),len(pwset),len(pwset1),len(pwset2), len(selected_pws), len(pwset1)+len(pwset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'iittukaunisto', 1)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwset1[0], allpw_counter[pwset1[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29587 7346 22241\n",
      "0.286656259479\n"
     ]
    }
   ],
   "source": [
    "# just to compare set and list iteration complexity\n",
    "start = timeit.default_timer()\n",
    "\n",
    "pwfinal1=[(item, pw_counter[item], '', '',sourcename) for item in pwset1] # allpw_counter --> pw_counter\n",
    "\n",
    "pwfinal2=[(item, pw_counter[item], oned[item][0], oned[item][-1],sourcename) for item in pwset2]\n",
    "\n",
    "pwfinal = pwfinal1 + pwfinal2\n",
    "\n",
    "print len(pwfinal), len(pwfinal1), len(pwfinal2)\n",
    "\n",
    "stop=timeit.default_timer()\n",
    "print stop-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7346, 22241)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pwfinal1), len(pwfinal2)#, len(pwfinal3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((u'aenaenNEANEA', 1, '', '', '78k'),\n",
       " (u'sonja', 7, u'sonja', u'sonja', '78k'),\n",
       " (u'aenaenNEANEA', 1, '', '', '78k'))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwfinal1[1],pwfinal2[1], pwfinal[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfmatch_names=pd.DataFrame(pwfinal, columns = [\"pw\", \"freq in pws\",\"combiword\",\"found words\",\"source\"])\n",
    "\n",
    "dfmatch_names['pw'] = np.where(dfmatch_names['pw'].str.contains(\",\"), dfmatch_names['pw'].str.replace(\",\", \".\"), dfmatch_names['pw'])\n",
    "dfmatch_names['pw'] = np.where(dfmatch_names['pw'].str.contains(\"\\\"\"), dfmatch_names['pw'].str.replace(\"\\\"\", \".\"), dfmatch_names['pw'])\n",
    "dfmatch_names['pw'] = np.where(dfmatch_names['pw'].str.contains(\"\\'\"), dfmatch_names['pw'].str.replace(\"\\'\", \".\"), dfmatch_names['pw'])\n",
    "dfmatch_names['pw'] = np.where(dfmatch_names['pw'].str.contains(\";\"), dfmatch_names['pw'].str.replace(\";\", \".\"), dfmatch_names['pw'])\n",
    "\n",
    "# len(dfmatch_names)\n",
    "# dfmatch_names = dfmatch_names[dfmatch_names['freq in pws']]\n",
    "# len(dfmatch_names)\n",
    "\n",
    "filename = \"tko_2027/data/\"+sourcename+\"_matchfreqfinnw_allpw.csv\"\n",
    "dfmatch_names.to_csv(filename, sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys  \n",
    "\n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io, json\n",
    "\n",
    "filename = \"tko_2027/data/\"+sourcename+\"_word_counter.txt\"\n",
    "with io.open(filename, 'w', encoding='utf-8') as f:\n",
    "  f.write(unicode(json.dumps(wordsum_dict, ensure_ascii=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import io, json\n",
    "\n",
    "# filename = \"tko_2027/data/\"+sourcename+\"_word_counter.txt\"\n",
    "# with io.open(filename, 'w', encoding='utf-8') as f:\n",
    "#   f.write(unicode(json.dumps(word_pwfreq, ensure_ascii=False)))\n",
    "\n",
    "filename = \"tko_2027/data/\"+sourcename+\"_allpw_counter.txt\"\n",
    "with io.open(filename, 'w', encoding='utf-8') as f:\n",
    "  f.write(unicode(json.dumps(allpw_counter, ensure_ascii=False)))\n",
    "\n",
    "filename = \"tko_2027/data/\"+sourcename+\"_pw_counter.txt\"\n",
    "with io.open(filename, 'w', encoding='utf-8') as f:\n",
    "  f.write(unicode(json.dumps(pw_counter, ensure_ascii=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
